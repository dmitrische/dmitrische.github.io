[
  {
    "objectID": "skillset/skillset.html",
    "href": "skillset/skillset.html",
    "title": "Skillset",
    "section": "",
    "text": "Here you will find a set of what I consider to be my core skills and competencies. Click on any one for more detailed context with links to relevant projects in my portfolio.\n\n\n\n\n\n\n\nComputational modelling\n\n\nUsing computers to simulate and study complex systems, ranging from nanoscale materials to market-based economies.\n\n\n\n\n\n\n\n\n\n\nSoftware development\n\n\nDesigning, implementing, testing, and documenting software in multiple programming languages.\n\n\n\n\n\n\n\n\n\n\nData analysis\n\n\nInspecting, cleaning, transforming, modeling, and interpreting data with the goal of discovering useful information.\n\n\n\n\n\n\n\n\n\n\nHigh-performance computing\n\n\nUsing supercomputers and computer clusters to solve resource-intensive computational problems.\n\n\n\n\n\n\n\n\n\n\nCommunication\n\n\nDescribing or explaining complex and specialized information to audiences who may or may not be familiar with the subject matter.\n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nSystematically pursuing answers to questions for the purposes of accumulating knowledge, solving problems, or developing products.\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "skillset/skills/02_SoftwareDev.html",
    "href": "skillset/skills/02_SoftwareDev.html",
    "title": "Software development",
    "section": "",
    "text": "If computational modelling is my craft, then computational methods and software packages are my tools; and I believe that tools should be shared and reused whenever possible.\nInitially, as a graduate student, I saw coding more as a means to an end and a potentially reusable by-product of my research activity, but with experience I became more appreciative of software carpentry and developed an interest in writing more robust code that others could use with ease."
  },
  {
    "objectID": "skillset/skills/02_SoftwareDev.html#early-experience",
    "href": "skillset/skills/02_SoftwareDev.html#early-experience",
    "title": "Software development",
    "section": "Early experience",
    "text": "Early experience\nAs a graduate student I’ve spent considerable effort implementing Bash and Perl scripts to pre-process input data and post-process the output from classical molecular dynamics (MD) simulations.\nHowever, my first serious programming project was in Fortran, and it consisted of two stages:\n\nimplementing a three-body interatomic potential for an in-house MD code; and\nparallelizing the entire code through atom-based decomposition and using Message Passing Interface (MPI).\n\nImplementing\nHaving developed my own models and methods in code and then passed it on for others to use, as well as having contributed to a shared codebase in a multiple research groups,\nAs a computational modeller who enjoys implementing own models and methods in code, as well as relying on tools developed by others, I value\nLess about a personal means of breaking down a problem, and more about building a reproducible product, often collaboratively as part of a group."
  },
  {
    "objectID": "skillset/skills/01_CompModel.html",
    "href": "skillset/skills/01_CompModel.html",
    "title": "Computational modelling",
    "section": "",
    "text": "I genuinely enjoy using computers to simulate and study complex systems, and it is this enjoyment that has motivated me most so far in my career. I still find this fact surprising, because I was initially dissuaded from programming as a first-year undergraduate. Fortunately, my subsequent summer internships at Industrial Research Ltd drastically changed my perspective, setting me on course for a PhD in physics with strong focus on nanoscale materials modelling.\nTo this day I see programming and implementing algorithms as an integral part of my process for breaking down complex problems into more manageable chunks. For instance, during my PhD, implementing my own Fortran subroutines in Fortran was instrumental in shedding new light on how metal nanoparticles can melt and fill a capillary. More recently, implementing an agent-based model of a minimal economy with Python, in an object-oriented paradigm, helped me develop a clearer understanding of microeconomic drivers of trade and how a price can emerge within a market setting.\nA laboratory for virtual experiments (simulation cartoon?)"
  },
  {
    "objectID": "portfolio/portfolio.html",
    "href": "portfolio/portfolio.html",
    "title": "Projects",
    "section": "",
    "text": "Here you will find links to some of my currently ongoing projects on GitHub, followed by a listing of past (mostly academic) work with links to retrospective summaries."
  },
  {
    "objectID": "portfolio/portfolio.html#in-progress",
    "href": "portfolio/portfolio.html#in-progress",
    "title": "Projects",
    "section": "In progress",
    "text": "In progress\n\nData gathering, analysis, and modelling of carbon market price dynamics. See repo and web-app deployed on AWS.\nPreliminary modelling and interactive calculation of carbon units earned from forestry in the NZ-ETS. See repo and Jupyter notebook deployed on Binder.\nPython implementation of an agent-based model of a minimal market-based economy developed by Steilgitz et al., with various extensions including emissions trading.\nPreliminary exploration of the public Crash Analysis System (CAS) data for New Zealand. See GitHub repo for details."
  },
  {
    "objectID": "portfolio/portfolio.html#past-work",
    "href": "portfolio/portfolio.html#past-work",
    "title": "Projects",
    "section": "Past work",
    "text": "Past work\n\n\n\n\n  \n\n\n\n\nT-matrix calculations\n\n\n\n\n\nDeveloping and testing custom software for T-matrix calculations of light scattering by assemblies of particles, using Fortran and Python.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nNanoalloys structure prediction\n\n\n\n\n\nDeveloping, benchmarking, and applying global optimisation algorithms for theoretical structure prediction of mixed metal clusters.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nGallium superatoms\n\n\n\n\n\nLooking for evidence of atomic-like orbitals and electron shell structure in metalloid gallium clusters, using quantum chemistry methods.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nNanoscale capillary action\n\n\n\n\n\nExplaining unexpected capillary absorption of metal nanodroplets, using molecular dynamics simulation and thermodynamic modelling.\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMelting of nanoparticles\n\n\n\n\n\nShedding new light on the melting behavior of metal nanoparticles, using molecular dynamics simulation and thermodynamic modelling.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dmitri Schebarchov",
    "section": "",
    "text": "GitHub\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Scholar\n  \n  \n    \n     Email\n  \n\n  \n  \nWelcome to my website!\nI am a computational modeller with PhD in physics and extensive postdoctoral experience in simulation-based research, mostly in the realm of nanoscale materials modelling.\nAfter joining a fin-tech startup, my interests broadened to include agent-based modelling of socio-economic systems: financial markets, emissions trading schemes, and blockchain networks.\nTo further expand my technical skills, I intend to pursue a number of data science projects and use this website for (i) blogging about my learnings and (ii) showcasing some of my work.\nPlease, feel free to download my CV, find out more about my skillset, explore my projects portfolio, read my blog, or reach out by e-mail."
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "Blog",
    "section": "",
    "text": "Here you will find short pieces of writing summarizing isolated or immature bits of work, such as early prototypes or preliminary analyses, or even just initial thoughts on a subject matter."
  },
  {
    "objectID": "blog/blog.html#posts",
    "href": "blog/blog.html#posts",
    "title": "Blog",
    "section": "Posts",
    "text": "Posts\n\n\n\n\n  \n\n\n\n\nWeb-scraping for \"carbon\"\n\n\n\n\n\nUsing Python to gather historical data on the market price of NZUs.\n\n\n\n\n \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/posts/001_nzu_price_scrape.html",
    "href": "blog/posts/001_nzu_price_scrape.html",
    "title": "Web-scraping for \"carbon\"",
    "section": "",
    "text": "This inaugural blog-post describes my first experience in web-scraping, which involved obtaining historical price data from publicly visible snippets of a pay-walled news website. The exercise was partly motivated by @theecanmole’s GitHub repository containing manually “web-scraped” New Zealand Unit (NZU) price data from 2010 onwards. I wanted to automate the process by implementing a scraper, mostly because it seemed like a good exercise and, to the best of my knowledge, nobody else had done it before and made the code publicly available. So, feel free to look at and reuse my code, but I will not dwell on it here. Instead, I will just outline my approach in words, after providing a bit more context and dispelling any legal concerns; and I will compare my scraped data with @theecanmole’s."
  },
  {
    "objectID": "blog/posts/001_nzu_price_scrape.html#nzu-trading",
    "href": "blog/posts/001_nzu_price_scrape.html#nzu-trading",
    "title": "Web-scraping for \"carbon\"",
    "section": "NZU trading",
    "text": "NZU trading\nNew Zealand operates a domestic emissions trading scheme (NZ-ETS) with NZU as the standardized tradeable unit (permitting the holder to emit one tonne of CO2-equivalent). While the primary market for NZUs started functioning only in 2021, when the first public auctions were held to set the price for newly-issued NZUs, unused NZUs have been resold on platforms like CarbonMatch, CommTrade, and even TradeMe since 2010 (if not earlier). While this secondary market for NZUs has been developing for over a decade now, Carbon News and other news services have been providing comprehensive daily information on the progress, including regular updates on the market price movement."
  },
  {
    "objectID": "blog/posts/001_nzu_price_scrape.html#carbon-news",
    "href": "blog/posts/001_nzu_price_scrape.html#carbon-news",
    "title": "Web-scraping for \"carbon\"",
    "section": "Carbon News",
    "text": "Carbon News\nCarbon News is described as “New Zealand’s only daily news service covering the carbon markets, climate change, sustainable business and the growth of the low-carbon economy”. It is a private business, so to read the published stories in full one must pay a subscription fee. However, some useful information can still be gleaned from headlines and short summaries visible to non-subscribers.\nThe Jarden NZ Market Report section lists recent stories focusing on the price of NZUs traded on CommTrade. Each story (such as this one) reports on the latest “fixing”, i.e. the spot price, as well as the opening bid and offer prices.\n\n\n\n\n\nThe price history plotted in the image accompanying each story covers only the previous six months, whereas Jarden NZ Market Report’s archive dates back to 2008. To graph and analyze the entire price history, one could scrape all the publicly visible NZU prices with the corresponding dates, and save this data to a local file, say a CSV file with the following format:\ndate,price\n2023-07-24,47.25\n2023-07-25,50.00"
  },
  {
    "objectID": "blog/posts/001_nzu_price_scrape.html#legal-concerns",
    "href": "blog/posts/001_nzu_price_scrape.html#legal-concerns",
    "title": "Web-scraping for \"carbon\"",
    "section": "Legal concerns",
    "text": "Legal concerns\nCarbon News website’s robots.txt indicates that only a small number of user-agents have been explicitly disallowed from crawling, whereas everyone else should just use a crawl-delay of two seconds. This allowance suggests that the website creators do not object to the publicly visible content being scraped, and I did not find any statements in the Policies and Service sections that would point to the contrary. Hence, to the best of my knowledge and understanding, my scraping exercise does not entail illegal.\nI am aware that CommTrade claim some copyright, stating that “reproduction of any data or images on this site cannot be used without the permission of Jarden.” In fact, when I first became interested in historical data on the price of NZUs, I’ve actually emailed carbon@jarden.co.nz asking for it, and they offered it to me for a fee of $5k. However, I was not willing to pay this much (if anything), and I did not feel like bargaining. Instead, I turned this obstacle into an opportunity to develop my scraping skills, using a different website as my source. Later I felt vindicated after finding a report from Exchange Data International (EDI), which concludes:\n\n“There is no credible basis for any stock exchange in the EU (or the US) to claim copyright in closing prices or indices. Even by the most indulgent standards, closing prices are below the standards of originality and substantiality required under copyright legislation, regardless of whether they are the product of algorithms or the actual final trading price of the day.”\n\nI think I agree with this conclusion and hope Jarden CommTrade rethink their data policy, if they haven’t already. In any case, back to legal scraping…"
  },
  {
    "objectID": "blog/posts/001_nzu_price_scrape.html#content-inspection",
    "href": "blog/posts/001_nzu_price_scrape.html#content-inspection",
    "title": "Web-scraping for \"carbon\"",
    "section": "Content inspection",
    "text": "Content inspection\nIn general, data scraping involves parsing the HTML source code of a web page of interest, and in the present case we have two to begin with: Jarden NZ Market Report and its archive. By scrolling through these two webpages one can glean all the information required to produce the desired CSV file: the date, the price, and the story URL.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSomewhat inconveniently, the date formatting is variable: showing just “Today” or the appropriate weekday (e.g. Thursday) for stories that are less than a week old, and the date in full (e.g. “25 Jul 23”) stated only for older stories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMuch more inconvenient is the variation in how the spot price has been reported over time, systematically featuring in the headlines only since 22 Feb 2016. Extracting the spot price from hundreds of older archived stories actually requires opening and parsing the stories’ individual web pages. Adding to the inconvenience is that the format of the publicly visible summaries changed fairly inconsistently over time, especially in early years, raising the question of whether it is even possible to extract all the right price values using relatively simple algorithm logic.\nUsing the browser’s inspection tool to examine the HTML source code of the Jarden NZ Market Report, we find a fairly tractable structure. The central listing of stories is associated with a &lt;div&gt; element of class \"StoryList\". Inside this element, the latest headline in the listing is associated with the element tagged by &lt;h1&gt;, the following six headlines are each tagged by &lt;h2&gt;, and the remaining ones by &lt;h3&gt;; and all these elements have the same class name \"Headline\" attributed to them. The actual headline text is nested inside an &lt;a&gt; sub-element with an href attribute (providing a URL fragment directing to the full story). Furthermore, each and every headline element is followed by an accompanying &lt;p&gt; element containing the story’s brief summary.\nInspection of the archive shows continuation of the same general pattern: first twenty stories in the archive are associated with &lt;h3&gt; elements (containing the headline) and accompanying &lt;p&gt; elements (containing the summary); while all older stories are tagged by &lt;h4&gt; and are not accompanied by &lt;p&gt; elements. For these older archived stories, the full date is embedded in the corresponding &lt;h4&gt; element but outside the internal &lt;a&gt; sub-element.\nHTML structure of each and every story’s web page is comparatively simple: the publicly visible text summary is spread over two consecutive &lt;div&gt; elements: one of class \"StoryFirstPara\"; and the other of class\"StorySecondPara\"."
  },
  {
    "objectID": "blog/posts/001_nzu_price_scrape.html#scraping-with-python",
    "href": "blog/posts/001_nzu_price_scrape.html#scraping-with-python",
    "title": "Web-scraping for \"carbon\"",
    "section": "Scraping with Python",
    "text": "Scraping with Python\nHaving gleaned the underlying HTML structure, I proceeded with the actual data scraping using Python in a Jupyter notebook. I did it in three stages.\nFirst I scraped the date, headline, and URL for all the stories listed in the Jarden NZ Market Report and its archive. I used a Python package called Beautiful Soup to parse the HTML and extract the text with the three relevant pieces of information.\nThen I wrote a custom function called strings2date to convert all the date strings into date objects using Python’s datetime module. This conversion enabled me to discard all the stories published prior to 14 May 2010, because we know that updates on the spot price have not been reported before then.\nThe third and final stage involved extracting the spot price either from the headlines I have already just scraped, or from the text summaries scraped from the stories’ individual web pages, accessed via their respective URLs. I implemented the parsing logic for the headlines and the summaries in two different functions, parse_headline and parse_summary, with the latter being slightly more complicated, but both relying on standard string functions and searching for fairly simple patterns in the text.\nAgain, feel free to inspect my Jupyter notebook and file scraping_functions.py for implementation details. In the remainder of this post I will just focus on the result and compare it with @theecanmole’s raw data."
  },
  {
    "objectID": "blog/posts/001_nzu_price_scrape.html#dataset-comparison",
    "href": "blog/posts/001_nzu_price_scrape.html#dataset-comparison",
    "title": "Web-scraping for \"carbon\"",
    "section": "Dataset comparison",
    "text": "Dataset comparison\nFigure below shows a plot of 2145 datapoints (in red) that I’ve scraped from Carbon News, as well as a (blue) line tracing the 1653 datapoints in @theecanmole’s dataset.\n\n\n\n\n\nIt is reassuring to see that the two overlaid plots are almost indistinguishable, but not quite. Even though my dataset has more points in total, it is actually more sparse than @theecanmole’s for years up to and including 2013, when @theecanmole relied on multiple other sources.\nSurprisingly, my dataset contains no price values from 2012, even though some of @theecanmole’s datapoints for that year are sourced from Carbon News. While it is entirely possible that my text parsing logic has missed some values, it also appears to be the case that some of the older market updates have not actually been archived. For example, @theecanmole’s price value of $8.30 from 16 Dec 2011 cites a Carbon News article with unique storyID=5808 and a valid URL, but my crawling scraper failed to discover this URL, simply because the story is not listed on Jarden NZ Market Report Archive. This realization makes me even more appreciative of @theecanmole’s efforts."
  },
  {
    "objectID": "blog/posts/001_nzu_price_scrape.html#future-updates",
    "href": "blog/posts/001_nzu_price_scrape.html#future-updates",
    "title": "Web-scraping for \"carbon\"",
    "section": "Future updates",
    "text": "Future updates\nScraping NZU price history from Carbon News was intended as a one-off learning experience. However, I plan to regularly update my dataset as new daily prices are reported on the Jarden NZ Market Report. In another post I might write about how this updating process could be automated and deployed on AWS (using a free-tier EC2 instance), but until then I will simply be re-running the first two code cells of my Jupyter notebook."
  },
  {
    "objectID": "portfolio/projects/02_Nanofluidics.html",
    "href": "portfolio/projects/02_Nanofluidics.html",
    "title": "Nanoscale capillary action",
    "section": "",
    "text": "Figure: Sequence of snapshots showing thin metal film dewetting on a porous substrate."
  },
  {
    "objectID": "portfolio/projects/01_ClusterMelting.html",
    "href": "portfolio/projects/01_ClusterMelting.html",
    "title": "Melting of nanoparticles",
    "section": "",
    "text": "This project was partly motivated by the work at Nano Cluster Devices Limited – a Christchurch-based company making nanoscale devices by depositing metal nanoparticles on top of a grooved surface. Structural disintegration or misassembly due to melting was a concern to them, and it was more cost-effective to explore the issue using computer simulations before committing to more targeted (and relatively costly) experiments.\nMore motivation came from genuine curiosity and desire to learn how a fairly intuitive phenomenon, such as melting, can become ambiguous and counter-intuitive at the nanometre scale.\nSo, initially as a summer research student at Industrial Research Ltd (now Callaghan Innovation), I was tasked with running and analysing a series of simulations of model nanoparticles at gradually increasing temperatures, looking for anything out of the ordinary near the melting transition.\n\n\n\nFigure: Two snapshots of a model nanoparticle simulated at different temperatures, with the atoms color-coded by mobility: lighter-colored atoms are solid-like, and darker-colored atoms are liquid-like."
  },
  {
    "objectID": "portfolio/projects/01_ClusterMelting.html#context",
    "href": "portfolio/projects/01_ClusterMelting.html#context",
    "title": "Melting of nanoparticles",
    "section": "",
    "text": "This project was partly motivated by the work at Nano Cluster Devices Limited – a Christchurch-based company making nanoscale devices by depositing metal nanoparticles on top of a grooved surface. Structural disintegration or misassembly due to melting was a concern to them, and it was more cost-effective to explore the issue using computer simulations before committing to more targeted (and relatively costly) experiments.\nMore motivation came from genuine curiosity and desire to learn how a fairly intuitive phenomenon, such as melting, can become ambiguous and counter-intuitive at the nanometre scale.\nSo, initially as a summer research student at Industrial Research Ltd (now Callaghan Innovation), I was tasked with running and analysing a series of simulations of model nanoparticles at gradually increasing temperatures, looking for anything out of the ordinary near the melting transition.\n\n\n\nFigure: Two snapshots of a model nanoparticle simulated at different temperatures, with the atoms color-coded by mobility: lighter-colored atoms are solid-like, and darker-colored atoms are liquid-like."
  },
  {
    "objectID": "portfolio/projects/01_ClusterMelting.html#methods",
    "href": "portfolio/projects/01_ClusterMelting.html#methods",
    "title": "Melting of nanoparticles",
    "section": "Methods",
    "text": "Methods\nSimulations were carried out using several existing implementations of classical molecular dynamics (MD), which is a numerical method for stepping through Newton’s equations of motion for a given model system of interacting atoms.\nTo execute the simulations, I had to operate in a Linux environment and use command-line tools to launch jobs on a local computer cluster (through a queuing system).\nI learned shell scripting and how to build data pipelines with grep, sed, and awk, which enabled me to rapidly process and analyse large volumes of structured data produced by my simulations.\nThe analysis involved visual inspection and structural classification of 3D models, as shown in the figure above, as well as processing time-series data and using statistical techniques to calculate equilibrium averages."
  },
  {
    "objectID": "portfolio/projects/01_ClusterMelting.html#outputs",
    "href": "portfolio/projects/01_ClusterMelting.html#outputs",
    "title": "Melting of nanoparticles",
    "section": "Outputs",
    "text": "Outputs\nSome of the findings were indeed surprising! most notably that, in a partially molten cluster, the solid crystal-melt interface can cause\n\nsudden restructuring of the solid core; and\nsuper-heating, i.e. complete melting occurring at a temperature higher than the bulk melting temperature.\n\nThe latter observation was particularly counter-intuitive, because nanoparticles generally melt at lower temperatures compared to their bulk counterpart.\nFinally, the findings opened up new lines of inquiry that led to my PhD in physics, which .\n\n\n\nFigure: Metal nanoparticle on a graphene surface."
  },
  {
    "objectID": "portfolio/projects/01_ClusterMelting.html#acknowledgements",
    "href": "portfolio/projects/01_ClusterMelting.html#acknowledgements",
    "title": "Melting of nanoparticles",
    "section": "Acknowledgements",
    "text": "Acknowledgements"
  }
]